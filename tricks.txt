1. 结构里增加residual block， 试试ReLU6
2. learning rate， 0.1， warmup，consine
3. 数据预处理，去除cutout
4. bag of tricks
5. 随机初始化 vs. 现在的方法



对照组：huawei2实验4（0.950那个）
加了residual结构‘+操作’，train_acc提升很快到1.0，val_acc停在0.932，好像过拟合了
加了residual结构‘concat操作’，train_acc很快到1.0，val_acc停在0.933，好像过拟合了
所有relu改relu6，提升了一点点，到0.9515
xingzhi2最后一个block重复堆叠4次，train_acc提升会慢一点，434epoch，train_acc0.987，val_acc0.942也上不去了
xingzhi2最后一个block重复堆叠8次，train_acc0.93,640epoch,val_acc0.917
实验6搜出来的复杂结构进行堆叠4次，train_acc很快到1.0，600epoch,val_acc0.923
加warmup，加cosine学习率，初始学习率0.1，val_acc0.9558，初始学习率0.025，val_acc0.943
加relu6

在0.954那个上，初始学习率0.1，cos下降，0.946-0.947
在0.954那个上，初始学习率0.1，全改relu6，cos下降，0.957-0.959
在0.954那个上，初始学习率0.025，全改relu6，cos下降，0.9617
在0.954那个上，初始学习率0.025，cos下降，0.953

评估模块代码fix过两次bug之后，重新retrain，
在0.954那个上，初始学习率0.025，cutout,不改relu6，cos下降，0.963
在0.954那个上，初始学习率0.025，cutout,全改relu6，cos下降，0.965

重新基于0.950那个，

华为第三次拿回来的数据,4_1retrain结果0.961，nohup.out.hw3_4_1.lr.cos.0.025.cutout.new
                      4_9retrain结果0.958，nohup.out.hw3_4_9.lr.cos.0.025.cutout.relu6.new
                      retrain0.954那个，得hw3_4_9.lr.cos.0.025.cutout.relu6.new.cifar100，0.743
华为第四次拿回来的数据，nohup.out.hw4_4_1.lr.cos.0.025.cutout.new.cifar100，0.7314
华为第五次拿回来的数据，nohup.out.hw5_4_4.lr.cos.0.025.cutout.relu6.new.cifar100，0.739
		0.954的结果channel数翻倍之后nohup.out.0.954.lr.cos.0.025.relu6.cutout.new.channel.double，0.9656
自己设计网路结构，



采样失败的原因：采样多次的逻辑，采样多次时未查重，如果重复的编码评分都高，更新模型时都会被opt模块保存，充分不必要条件
因为self.follow_constraint(ins.get_features())这玩意儿的存在，去重机制一直没有用上，注释掉1142-1143行
多次采样的查重，采不到早停，不进行尝试了，现在是终止程序，因为早停之后传入不足spl_times的编码会导致对不齐

ub，40%，，50%
重构版本还没将ub改成按比例设置的逻辑

filter_size各block之间重叠一个，retrain的时候filter_size可以翻倍
跨层连接很多的话，每个节点的跨层连接数量可以减为1

task：：batch_size设置64，channel数减半，retrain翻倍

把参数和结果整合表格，会议上观看
channel数量翻倍，效果有一点点提升，0.9617->0.9656,提升了0.004
bag of tricks 杂七杂八的加一加，提了有四个点，

Understanding the difficulty of training deep feedforward neural networks这个讲参数初始化，，


代码重构的问题：
换成filter_size,新版只接驳一个
tf.global_variables_initializer()会不会覆盖前面的block
写下retrian代码
评估不在子进程的时候，报错在第二个block，train_winner第二个网络
uncertain_bit需要改成按照比例设置，按照数字设置
写日志这个，只需要在删除网络的时候，将网络信息记录下来，其他信息不需要，单独保存一个文件，参照原来的save_info能不能实现
主控需要额外一个日志记录网络的结构信息


记得预测模块的接口，传的pre_block是一个item_list,下个周期解决
block的filter_size由二维改成一维，下个周期解决

预测模块训练模型，要不要写到主控代码里，写的话什么时候训练，用什么样的数据训练，暂时不加

每轮每个网络多次采样的逻辑可以改成粒度更细的实现，每采一次更新一次模型，用event实现

对接口
返回的模型名字带编号，然后主控代码把编号去掉



添加数据的方式：乘比例系数

train_winner更新模型，不单独维护epoch

train_winner选出最高评分，加满数据，单独epoch
画图可带这个点

eva.set_epoch()
eva.set_data_size(int)

block数量变大，
max_branch_depth调到很大就好（block）
注意添加，sepearta卷积 
运行时间加到报告里面
枚举设置443，
日志再调

0.969

每轮的平均评估时间，每轮打印数据量，每个任务的搜出来的网络结构，每个拓扑结构单轮竞赛的多次采样的均分统计出来，网络结构

确认train_winner和普通搜索的数据、epoch数量都一样的话，就不额外定义eva对象了
retrain的问题，retrain的数据是50000


关于评估代码的问题:
（细节）0.1，梯度不更新的情况，来不及debug的话，可以先在末尾多加全连接，来避免0.1的情况
（细节）关于早停的判断逻辑，cifar100上，5个epoch大于0.05，cifar10上，5个epoch大于0.15，除retrain外均使用早停
末尾多加一层全连接，基本都可以涨起来，早停是不是要去掉

初始化如何与采样多次的逻辑融合，初始化次数和采样多次的次数保持一致，第一轮评估过后，每个网络更新多次，但是只采样一次，后面的轮数，更一次，采一次

关于主控代码的问题：
各block之间需不需要延续更新opt模型，就是把每个拓扑结构对应的spl对象（里面有opt对象），保留至下一个block继续使用，
这样opt模型的生命周期会更长
注明：各block的采样空间对应的filter_size不一样，其他都一样

加和、拼接操作可选，
竞赛时加入现有网络，一起竞赛，先只在global实验，block先不加
跟华为交付稳定代码
分割、视频去噪，超分辨


因为cifar100一般是比较深的网络，深度可以改成6或7，支链个数可以改成1或2，
比对下评估模块，和其他代码的时间差距，
测试一下retrain和evaluate两个函数对于同样的配置会不会有同样的结果，！

一起改评估模块的代码
提升cifar100在现有评估模块上的点数：
在现有retrain代码上评估之前搜出来的hw_5_4_4的结果，0.63
用原来的retrain评估一下相同的结果，

代码的鲁棒性，效率，
加注释，写清楚含义

12/9，444配置的卡在第四个block，round1，不报错，GPU长时间未使用（本地调试没有问题，eva_debug置为1）

数据处理，因为tfrecord不知道如何定义出入口的名字，目前是自己用python写的数据处理
看下乔康细粒度train_winner的实现
曲线图中那个陡峭的峰是因为做了finetune效果变好


只有一个用初始化，其他的纯采样，已改
算每个网络平均用时，直接从日志中取
调用子进程，加上try，catch功能，已加

复现错误，epoch调小，数据量调小，完整跑完一遍；重新从第四个block开始搜索，搜索过程结束，也没有遇到错误
confirm train加上用时，新加入的逻辑train_winner需要打印统计用时

在兴智repeat search

写一下检查各接口有效性的代码，重新整理一波
可以先在评估模块上复现densenet的结果，它们结构应该是最相似的
评估模块的retrain不支持repeat search构建网络，修改后retrain

arrange result添加打印block_num和轮数的逻辑



